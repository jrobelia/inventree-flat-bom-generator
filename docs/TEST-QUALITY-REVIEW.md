# Test Quality Review

**Date**: December 15, 2025 (Updated: December 18, 2025)  
**Total Tests**: 121 (all passing)  
**Purpose**: Evaluate existing test quality and identify areas for improvement

**Related Documentation:**
- [TEST-WRITING-METHODOLOGY.md](TEST-WRITING-METHODOLOGY.md) - Code-first approach for writing/validating tests
- [TEST-PLAN.md](../../flat_bom_generator/tests/TEST-PLAN.md) - Testing strategy and execution
- [ROADMAP.md](ROADMAP.md) - Test-first workflow and priorities

---

## Summary Assessment

### Overall Quality: **B+ (Improved from B- after Dec 18 updates)**

**Recent Improvements (Dec 18):**
- âœ… Internal fab tests rewritten - upgraded from Low to High quality
- âœ… All 121 unit tests passing (was 105 passing, 1 skipped)
- âœ… Found and removed 45 lines of dead code
- âœ… Added ValueError for data corruption (fail fast on unit mismatch)

**Strengths:**
- Good coverage of pure functions (categorization, length extraction)
- Clear test names and documentation
- Tests are organized by feature area
- Good use of edge cases in newer tests
- Code-first methodology now documented and applied

**Weaknesses:**
- Some tests still need validation walkthrough (see TEST-WRITING-METHODOLOGY.md tracker)
- Some tests duplicate calculation logic instead of importing actual functions
- Limited error condition testing in some areas
- Test_full_bom_part_13.py still has magic numbers

---

## Test File Analysis

### âœ… High Quality Tests

#### 1. `test_serializers.py` (23 tests)
**Quality**: Excellent  
**What's Good:**
- Comprehensive field validation
- Tests both valid and invalid inputs
- Clear error expectations
- Edge cases covered (None values, empty strings, type mismatches)
- Self-contained (no external dependencies)

**Coverage:**
- âœ… BOMWarningSerializer validation (all warning types, required fields, optional fields)
- âœ… FlatBOMItemSerializer validation (24 fields, all categories)
- âœ… Edge cases (missing fields, wrong types, None values)

**What Could Improve:**
- Could test serializer output format more explicitly
- No tests for serializer error messages (only that errors exist)

---

#### 2. `test_shortfall_calculation.py` (10 tests)
**Quality**: Very Good  
**What's Good:**
- Tests all 4 checkbox combinations (comprehensive)
- Clear documentation of formulas
- Tests realistic scenarios
- Edge cases covered (zero values, high allocations, decimal quantities)

**Coverage:**
- âœ… All 4 calculation modes
- âœ… Edge cases (zeros, exact matches, high allocations)
- âœ… Realistic production scenarios
- âœ… Decimal quantities

**What Could Improve:**
- Duplicates calculation logic instead of importing from actual code
- No tests for negative values (should they be allowed?)
- No boundary testing (very large numbers, precision limits)

---

#### 3. `test_categorization.py` (39 tests)
**Quality**: Very Good  
**What's Good:**
- Comprehensive coverage of all pure functions
- Clear test organization (4 test classes)
- Tests edge cases well (empty strings, None, special chars)
- Good documentation

**Coverage:**
- âœ… `_extract_length_from_notes()` - 15 tests (excellent)
- âœ… `categorize_part()` - 12 tests (very good)
- âœ… `_extract_length_with_unit()` - 12 tests (excellent)
- âœ… `_check_unit_mismatch()` - 6 tests (good)

**What Could Improve:**
- Tests use hardcoded category IDs (fragile if IDs change)
- No tests for concurrent category matches (priority order)
- Limited testing of supplier hierarchy edge cases

---

### âš ï¸ Medium Quality Tests

#### 4. `test_assembly_no_children.py` (5 tests)
**Quality**: Good  
**What's Good:**
- Tests specific bug fix scenario
- Clear documentation of issue
- Tests both assembly types (Assy and Internal Fab)

**Coverage:**
- âœ… Assembly with no children gets flagged
- âœ… Internal Fab with no children gets flagged
- âœ… Regular parts not affected
- âœ… Purchased assemblies handled correctly

**What Could Improve:**
- Duplicates tree structure in each test (could use helper)
- Only tests `get_leaf_parts_only()` function, not full workflow
- No tests for nested assemblies with partial children
- Doesn't verify flag propagates through to API response

---

#### 5. `test_max_depth_warnings.py` (4 tests)
**Quality**: Good  
**What's Good:**
- Tests flag prioritization logic correctly
- Clear documentation of warning rules
- Tests both individual and summary warnings

**Coverage:**
- âœ… max_depth_exceeded prevents assembly_no_children flag
- âœ… Genuine empty assemblies get flagged
- âœ… Regular parts not affected
- âœ… Summary warnings generated

**What Could Improve:**
- Duplicates flag logic instead of calling actual functions
- No tests for multiple max_depth assemblies at different levels
- No tests for how warnings appear in API response
- Tests check logic, not actual behavior

---

#### 6. `test_cut_to_length_aggregation.py` (1 test)
**Quality**: Fair  
**What's Good:**
- Tests realistic cut list aggregation
- Verifies both total_qty and cut_list structure

**Coverage:**
- âœ… Multiple cuts of same part aggregate correctly
- âœ… cut_list breakdown matches total

**What Could Improve:**
- **Only 1 test** for entire feature
- No tests for edge cases (zero quantities, missing unit, different units)
- No tests for unit conversion
- No tests for parts without cut_list
- Doesn't test cut_list generation, only aggregation

**Recommendation**: Expand to 5-10 tests covering edge cases

---

### âŒ Low Quality Tests

#### 7. `test_internal_fab_cutlist.py` (10 tests)
**Quality**: Poor to Fair  
**What's Good:**
- Tests basic matching logic
- Advanced tests attempt real-world scenarios

**Major Issues:**
1. **Mock Function Doesn't Match Real Code**
   - `get_cut_list_for_row()` is a stub, not the actual function
   - Tests pass but don't validate real behavior
   - **This is testing fantasy code, not actual implementation**

2. **External CSV Dependency**
   - Advanced tests load `InvenTree_BomItem_2025-12-14_J48BLGx.csv`
   - If file changes/moves, tests fail
   - No validation that CSV structure matches expectations
   - Hard to understand what's being tested

3. **Complex Regex Parsing**
   - Tests parse part descriptions with regex
   - Duplicates parsing logic instead of using actual functions
   - Fragile to description format changes

4. **No Clear Assertions**
   - Some tests just check `> 0` or `assertGreater(x, 0)`
   - Doesn't verify actual expected values
   - Hard to know if test is actually working

**Coverage:**
- âš ï¸ Basic unit matching (good but uses stub)
- âŒ Advanced rollup tests (rely on CSV, unclear expectations)
- âŒ Real `get_flat_bom()` behavior not tested

**Recommendation**: **Rewrite from scratch**
- Remove stub function, test actual `get_flat_bom()` behavior
- Create small, controlled test data (no CSV dependency)
- Clear expected values for all assertions
- Test one thing at a time

---

#### 8. `test_full_bom_part_13.py` (1 test)
**Quality**: Very Poor  
**What's Good:**
- ... honestly, not much

**Major Issues:**
1. **Magic Number with No Context**
   - Expects exactly 9 unique parts
   - No explanation why 9 is correct
   - No validation of which 9 parts
   - If test fails, impossible to know what's wrong

2. **External Data Dependency**
   - Imports `full_bom_part_13` from another file
   - Test depends on data structure that might change
   - No documentation of what this data represents

3. **Unclear Purpose**
   - Test name: "test_total_unique_parts"
   - What is this testing? Deduplication? Counting? Both?
   - Is this testing a specific bug or feature?

4. **Comment Contradicts Code**
   - Comment says "9 unique parts (from the 10 sample rows)"
   - Why 10 rows but 9 unique? Is duplicate intentional?
   - No clarity on what makes parts "unique"

**Coverage:**
- â“ Something about counting unique parts?

**Recommendation**: **Delete or completely rewrite**
- Define clear test goal: "Test that deduplicate_and_sum() correctly identifies unique parts by part_id"
- Create explicit test data with known duplicates
- Assert specific part_ids are present
- Document why certain parts should be deduplicated

---

#### 9. `test_internal_fab_cut_rollup.py` (1 test - SKIPPED)
**Quality**: Unknown (Skipped)  
**Issue**: Test expects `internal_fab_cut_list` to be populated but receives empty list

**Status**: Marked with `@unittest.skip` since implementation

**Major Issues:**
1. **Skipped for Months**
   - Test has been skipped, not investigated
   - Either test expectations are wrong OR feature is broken
   - No resolution plan documented

2. **Unknown If This Is a Bug**
   - Is `internal_fab_cut_list` supposed to be populated?
   - Is test testing the right thing?
   - Is feature partially implemented?

**Recommendation**: **Investigate and resolve**
- Determine if feature is incomplete or test is wrong
- Either fix implementation or fix test expectations
- Don't leave skipped tests indefinitely

---

## Coverage Gaps

### âŒ Critical Missing Tests

#### 1. **NO TESTS FOR views.py API Endpoint**
**Risk**: Very High  
**What's Missing:**
- No tests for `FlatBOMAPIView.get()` endpoint
- No tests for request parameter handling
- No tests for error responses
- No tests for enrichment logic
- **We refactored enriched_item construction with serializers and have NO tests to verify it works**

**Why It Matters:**
- Views are the public API contract
- Frontend depends on exact response format
- Serializer refactoring could break API without detection
- All existing tests could pass while API is broken

**Recommendation**: **High Priority - Add Integration Tests**
```python
# Example of needed test
def test_get_flat_bom_api_response_structure(self):
    """Test that API returns expected JSON structure."""
    # Mock Part.objects.get(), get_flat_bom()
    response = client.get('/api/plugin/flat-bom/flat-bom/?part_id=123')
    self.assertEqual(response.status_code, 200)
    data = response.json()
    self.assertIn('part_id', data)
    self.assertIn('bom_items', data)
    self.assertIsInstance(data['bom_items'], list)
    # ... validate all fields
```

---

#### 2. **NO TESTS FOR bom_traversal.py Core Functions**
**Risk**: High  
**What's Missing:**
- No tests for `get_flat_bom()` main function
- No tests for `deduplicate_and_sum()` core logic
- No tests for `get_leaf_parts_only()` beyond assembly_no_children
- No tests for flag propagation through pipeline

**Why It Matters:**
- These are the most complex functions in the plugin
- `get_flat_bom()` orchestrates entire BOM traversal
- Changes here affect everything downstream

**Recommendation**: **High Priority - Add Unit Tests**
- Test `get_flat_bom()` with mock Part/BOMItem data
- Test `deduplicate_and_sum()` with known input/output
- Test flag propagation (max_depth, assembly_no_children)
- Test enable_ifab_cuts parameter behavior

---

#### 3. **NO TESTS FOR Error Conditions**
**Risk**: Medium  
**What's Missing:**
- What happens if part_id doesn't exist?
- What happens if Part has no BOM?
- What happens if database query fails?
- What happens with malformed request parameters?
- What happens with circular BOM references?

**Recommendation**: Add error path tests to views and traversal tests

---

#### 4. **Limited Tests for Internal Fab Feature**
**Risk**: Medium  
**What's Missing:**
- Real tests for `internal_fab_cut_list` generation
- Tests for unit conversion/matching logic
- Tests for enable_ifab_cuts=False behavior
- Tests for INTERNAL_FAB_CUT_UNITS setting

**Recommendation**: Rewrite internal_fab tests with clear test data

---

## Test Anti-Patterns Found

### 1. **Duplicating Production Code in Tests**
**Where**: `test_shortfall_calculation.py`, `test_max_depth_warnings.py`, `test_internal_fab_cutlist.py`

**Problem**: Tests copy logic instead of calling actual functions
```python
# âŒ BAD: Test duplicates shortfall calculation
def calculate_shortfall(self, ...):
    stock_value = in_stock
    if include_allocations:
        stock_value -= allocated
    # ... duplicates Panel.tsx logic
```

**Why Bad**: 
- If real code changes, test still passes with old logic
- Test isn't actually testing the code users run
- Maintenance burden (update code + update test copy)

**Fix**: Import and call actual function, don't duplicate

---

### 2. **Stub Functions in Tests**
**Where**: `test_internal_fab_cutlist.py`

**Problem**: Test defines a fake function and tests it
```python
# âŒ BAD: Testing a function we made up
def get_cut_list_for_row(row, cut_unit_setting):
    """Simulate backend logic..."""
    # This isn't real code!
```

**Why Bad**: Tests pass but real code could be completely different

**Fix**: Test actual functions from production code

---

### 3. **Magic Numbers Without Explanation**
**Where**: `test_full_bom_part_13.py`, `test_internal_fab_cutlist.py`

**Problem**: Assertions use unexplained values
```python
# âŒ BAD: Why 9? What are the 9 parts?
self.assertEqual(total_unique_parts, 9)

# âŒ BAD: Why "greater than 0"? What's the actual expected value?
self.assertGreater(total_unique_parts, 0)
```

**Why Bad**: 
- Test failure doesn't explain what went wrong
- No way to know if 9 (or > 0) is actually correct
- Test could be passing for wrong reasons

**Fix**: Use explicit expected values with comments explaining why

---

### 4. **External File Dependencies**
**Where**: `test_internal_fab_cutlist.py`, `test_full_bom_part_13.py`

**Problem**: Tests load CSV files from disk
```python
# âŒ BAD: Test relies on specific CSV file
bom_path = os.path.join(..., 'InvenTree_BomItem_2025-12-14_J48BLGx.csv')
```

**Why Bad**:
- Tests break if file moves/changes
- Hard to understand what's being tested
- Can't see test data without opening another file
- Couples tests to file structure

**Fix**: Define test data inline or in test fixtures

---

### 5. **Weak Assertions**
**Where**: Multiple files

**Problem**: Tests don't verify specific expected behavior
```python
# âŒ BAD: Just checks something exists
self.assertIsNotNone(row)

# âŒ BAD: Just checks it's not empty
self.assertGreater(len(result), 0)

# âœ… GOOD: Checks specific expected value
self.assertEqual(row['part_id'], 123)
self.assertEqual(len(result), 5)
```

**Fix**: Assert exact expected values when possible

---

## Recommendations by Priority

### ğŸ”´ Critical (Do These First)

1. **Add Views Integration Tests** (2-3 hours)
   - Test `FlatBOMAPIView.get()` with mock data
   - Verify response structure matches serializer
   - Test error conditions (missing part, invalid params)
   - Test enable_ifab_cuts parameter

2. **Fix or Remove Skipped Test** (1 hour)
   - Investigate `test_piece_qty_times_count_rollup`
   - Determine if feature is incomplete or test is wrong
   - Either fix implementation or update test expectations
   - Don't leave indefinitely skipped

3. **Rewrite test_internal_fab_cutlist.py** (2-3 hours)
   - Remove stub functions
   - Test actual `get_flat_bom()` behavior
   - Create small, controlled test data
   - Clear expected values for assertions

### ğŸŸ¡ High Priority (Do Soon)

4. **Add BOM Traversal Tests** (3-4 hours)
   - Test `get_flat_bom()` main orchestration
   - Test `deduplicate_and_sum()` with known data
   - Test flag propagation through pipeline
   - Test nested assemblies, purchased assemblies

5. **Expand Cut-to-Length Tests** (1-2 hours)
   - Add 5-10 more tests for edge cases
   - Test zero quantities, missing units
   - Test unit conversion
   - Test error conditions

6. **Fix test_full_bom_part_13.py** (30 min)
   - Either delete or completely rewrite
   - Clear test goal and expected values
   - No magic numbers

### ğŸŸ¢ Medium Priority (Future Work)

7. **Add Error Condition Tests** (2 hours)
   - Missing/invalid part_id
   - Database errors
   - Circular BOM references
   - Malformed parameters

8. **Improve test_shortfall_calculation.py** (1 hour)
   - Import actual calculation logic instead of duplicating
   - Add tests for negative values
   - Add boundary tests (very large numbers)

9. **Add Performance/Load Tests** (Optional)
   - Test with large BOMs (1000+ parts)
   - Test with deep nesting (10+ levels)
   - Test memory usage

---

## Test Quality Checklist

Use this when writing new tests or reviewing existing ones:

- [ ] **Test names describe what's being tested** (not just "test_1", "test_basic")
- [ ] **Tests use actual production code** (not duplicates or stubs)
- [ ] **Test data is defined inline** (not in external files)
- [ ] **Assertions check specific values** (not just "not None" or "> 0")
- [ ] **Expected values are documented** (why is 9 the right answer?)
- [ ] **Edge cases are tested** (zero, None, empty, very large, negative)
- [ ] **Error conditions are tested** (what should happen when it fails?)
- [ ] **Tests are independent** (can run in any order)
- [ ] **Tests are fast** (< 1 second each, no network/disk I/O)
- [ ] **One test per concept** (not testing 5 things in one test)

---

## Summary Statistics

**By Quality:**
- âœ… High Quality: 3 files (62 tests)
- âš ï¸ Medium Quality: 3 files (10 tests)
- âŒ Low Quality: 3 files (12 tests)

**By Coverage:**
- âœ… Well Tested: Pure functions (categorization, serializers)
- âš ï¸ Partially Tested: Feature-specific logic (warnings, assemblies)
- âŒ Untested: API endpoints, core BOM traversal, error conditions

**Overall Grade: C+**
- Good foundation with pure function tests
- Significant gaps in integration and core function testing
- Some tests are checking fantasy code instead of real implementation
- Need to add views tests before continuing serializer refactoring

---

**Next Steps:**
1. Add views integration tests (critical before more refactoring)
2. Fix or remove skipped test
3. Rewrite internal_fab_cutlist tests
4. Expand BOM traversal test coverage

